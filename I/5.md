### 可维护性与扩展性 — 分布式任务调度

- 我实际应用的组合架构其实非常简单，透过Redis的帮助去创建一个flag，当某架机器执行的时候会创建缓存，当缓存不在的话就代表能接着处理。
- 这个是一个不稳定，可是能暂时使用的一个任务定时器，而且没法完整的处理上述场景。
- 我个人推荐一套适合以上场景的架构是：***K8s CronJob + etcd Leader 选举 + Worker 拉取任务执行 的组合架构***

### K8s CronJob + etcd Leader 选举 + Worker 拉取任务执行 的组合架构：

- 使用etcd的选举模块可以解决容灾问题，当leader挂点，会有下一个新的leader被选举出来。
- leader的作用是用来分发任务，这个可以保证去重。
- 幂等性的话，以防万一建议由代码层面创建唯一标识，确保不重复。
- 扩容的话k8s可以直接扩容worker。

### main.go
```go
package main

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"time"

	"github.com/redis/go-redis/v9"
	clientv3 "go.etcd.io/etcd/client/v3"
	"go.etcd.io/etcd/client/v3/concurrency"
)

var rdb = redis.NewClient(&redis.Options{
	Addr: "localhost:6379",
})
var redisCtx = context.Background()

var etcdClient *clientv3.Client

type Task struct {
	TaskID   string `json:"task_id"`
	ExecTime string `json:"exec_time"`
}

func main() {
	var err error
	etcdClient, err = clientv3.New(clientv3.Config{
		Endpoints:   []string{"localhost:2379"},
		DialTimeout: 5 * time.Second,
	})
	if err != nil {
		log.Fatal("etcd连接失败:", err)
	}

	nodeID := fmt.Sprintf("node-%d", time.Now().UnixNano())

	// 启动 Leader 选举 + Worker 执行
	go startLeaderElection(nodeID)
	go startWorkerLoop()

	select {} // 阻塞主线程
}

func startLeaderElection(nodeID string) {
	session, err := concurrency.NewSession(etcdClient, concurrency.WithTTL(10))
	if err != nil {
		log.Fatal("etcd session 创建失败:", err)
	}
	defer session.Close()

	election := concurrency.NewElection(session, "/scheduler/leader")

	ctx := context.Background()
	err = election.Campaign(ctx, nodeID)
	if err != nil {
		log.Fatal("竞选 leader 失败:", err)
	}

	log.Println("[Leader] 当前成为调度中心 Leader:", nodeID)

	for {
		now := time.Now().Format("2006-01-02T15:04")
		dispatchTaskOnce("demo_task", now)
		time.Sleep(60 * time.Second)
	}
}

func dispatchTaskOnce(taskID, execTime string) {
	// 分布式锁，避免重复调度
	key := "/task-locks/" + taskID + "/" + execTime
	session, _ := concurrency.NewSession(etcdClient, concurrency.WithTTL(5))
	mutex := concurrency.NewMutex(session, key)

	if err := mutex.TryLock(context.Background()); err != nil {
		log.Println("[Leader] 任务已被其他节点处理:", taskID, execTime)
		return
	}
	defer mutex.Unlock(context.Background())

	task := Task{TaskID: taskID, ExecTime: execTime}
	data, _ := json.Marshal(task)
	_ = rdb.LPush(redisCtx, "task_queue", data).Err()

	log.Println("[Leader] 任务已发布:", taskID, execTime)
}

func startWorkerLoop() {
	for {
		result, err := rdb.BRPop(redisCtx, 5*time.Second, "task_queue").Result()
		if err != nil || len(result) < 2 {
			continue
		}

		var task Task
		if err := json.Unmarshal([]byte(result[1]), &task); err != nil {
			log.Println("任务解析失败:", err)
			continue
		}

		consumeAndExecuteTask(task.TaskID, task.ExecTime)
	}
}

var executedMap = make(map[string]bool) // 真实应用应使用 Redis/DB 存储

func alreadyExecuted(key string) bool {
	return executedMap[key]
}

func markExecuted(key string) {
	executedMap[key] = true
}

func consumeAndExecuteTask(taskID, execTime string) {
	uniqueKey := taskID + ":" + execTime

	if alreadyExecuted(uniqueKey) {
		log.Println("[Worker] 跳过重复任务:", uniqueKey)
		return
	}

	log.Println("[Worker] 开始执行任务:", uniqueKey)
	err := doBusinessLogic(taskID)
	if err != nil {
		log.Println("[Worker] 执行失败:", err)
		return
	}

	markExecuted(uniqueKey)
	log.Println("[Worker] 任务完成:", uniqueKey)
}

// 模拟业务逻辑
func doBusinessLogic(taskID string) error {
	log.Println("[Worker] 执行业务逻辑中:", taskID)
	return nil
}
```